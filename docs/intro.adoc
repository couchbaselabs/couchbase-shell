== Introduction

Couchbase Shell is fully featured, so it does not only contain commands related to Couchbase but is actually built on top of a
general purpose shell called https://www.nushell.sh/[Nushell]. This allows you to interact with the file system or any other
command available on your machine, making it a great tool for both operational and development tasks on top of Couchbase.

The following introduction only touches on the basic concepts to make you productive quickly. We recommend also checking out the
great https://www.nushell.sh/book[Nushell documentation] so you can get the most out of it.

=== Navigating the Shell

Commands take inputs and produce output in a structured manner, most often represented as tables. Note how both the generic `ls`
command and the Couchbase-specific `buckets` command both produce a table as their output:

```
> ls
╭────┬─────────────────────┬──────┬───────────┬───────────────╮
│  # │        name         │ type │   size    │   modified    │
├────┼─────────────────────┼──────┼───────────┼───────────────┤
│  0 │ CHANGELOG.md        │ file │   8.7 KiB │ 2 months ago  │
│  1 │ Cargo.lock          │ file │ 194.6 KiB │ a day ago     │
│  2 │ Cargo.toml          │ file │   2.7 KiB │ a day ago     │
│  3 │ Dockerfile          │ file │     590 B │ 2 months ago  │
│  4 │ LICENSE             │ file │  11.1 KiB │ 2 months ago  │
│  5 │ LICENSE_AGREEMENT   │ file │  43.7 KiB │ 2 months ago  │
│  6 │ NOTICES             │ file │ 221.6 KiB │ a week ago    │
│  7 │ README.md           │ file │   9.4 KiB │ 2 months ago  │
│  8 │ build               │ dir  │      64 B │ 4 weeks ago   │
│  9 │ docs                │ dir  │     608 B │ 3 minutes ago │
│ 10 │ examples            │ dir  │     192 B │ 4 months ago  │
│ 11 │ jupyter             │ dir  │     128 B │ 4 months ago  │
│ 12 │ rust-toolchain.toml │ file │     524 B │ 2 weeks ago   │
│ 13 │ src                 │ dir  │     416 B │ a day ago     │
│ 14 │ target              │ dir  │     352 B │ 2 weeks ago   │
│ 15 │ tests               │ dir  │     160 B │ 4 months ago  │
╰────┴─────────────────────┴──────┴───────────┴───────────────╯
```
[options="nowrap"]
```
> buckets
╭───┬─────────┬───────────────┬───────────┬──────────┬──────────────────────┬───────────┬───────────────┬────────┬───────╮
│ # │ cluster │     name      │   type    │ replicas │ min_durability_level │ ram_quota │ flush_enabled │ status │ cloud │
├───┼─────────┼───────────────┼───────────┼──────────┼──────────────────────┼───────────┼───────────────┼────────┼───────┤
│ 0 │ default │ beer-sample   │ couchbase │        1 │ none                 │ 200.0 MiB │ false         │        │ false │
│ 1 │ default │ default       │ couchbase │        2 │ none                 │ 100.0 MiB │ true          │        │ false │
│ 2 │ default │ travel-sample │ couchbase │        2 │ none                 │ 200.0 MiB │ false         │        │ false │
╰───┴─────────┴───────────────┴───────────┴──────────┴──────────────────────┴───────────┴───────────────┴────────┴───────╯
```

You can pipe the output into other commands, for example if you only want to see buckets that have `sample` in their name you
can utilize the `where` command:

[options="nowrap"]
```
> buckets | where name =~ "sample"
╭───┬─────────┬───────────────┬───────────┬──────────┬──────────────────────┬───────────┬───────────────┬────────┬───────╮
│ # │ cluster │     name      │   type    │ replicas │ min_durability_level │ ram_quota │ flush_enabled │ status │ cloud │
├───┼─────────┼───────────────┼───────────┼──────────┼──────────────────────┼───────────┼───────────────┼────────┼───────┤
│ 0 │ default │ beer-sample   │ couchbase │        1 │ none                 │ 200.0 MiB │ false         │        │ false │
│ 1 │ default │ travel-sample │ couchbase │        2 │ none                 │ 200.0 MiB │ false         │        │ false │
╰───┴─────────┴───────────────┴───────────┴──────────┴──────────────────────┴───────────┴───────────────┴────────┴───────╯
```

In a similar fashion you can turn this structured table into other output formats, for example JSON:

```
> buckets | where name =~ "sample" | to json
[
  {
    "cluster": "default",
    "name": "beer-sample",
    "type": "couchbase",
    "replicas": 1,
    "min_durability_level": "none",
    "ram_quota": 209715200,
    "flush_enabled": false,
    "status": "",
    "cloud": false
  },
  {
    "cluster": "default",
    "name": "travel-sample",
    "type": "couchbase",
    "replicas": 2,
    "min_durability_level": "none",
    "ram_quota": 209715200,
    "flush_enabled": false,
    "status": "",
    "cloud": false
  }
]
```

Exactly this type of composition takes the unix philosophy of "do one thing well" and meshes it together with the idea of flexible
structured pipelines. This allows to build powerful compositions that help you in your daily operations with Couchbase, both from
a developer or operations point of view.

=== Getting Help

Other than using this documentation for help, each command can be called with `-h` or `--help` to get information about potential
flags, arguments and subcommands. Also, some commands provide additional examples.

```
> buckets -h
Perform bucket management operations

Usage:
  > buckets {flags}

Subcommands:
  buckets config - Shows the bucket config (low level)
  buckets create - Creates a bucket
  buckets drop - Drops buckets through the HTTP API
  buckets flush - Flushes buckets through the HTTP API
  buckets get - Fetches buckets through the HTTP API
  buckets load-sample - Load a sample bucket
  buckets update - Updates a bucket

Flags:
  -h, --help - Display the help message for this command
  --clusters <String> - the clusters which should be contacted
```

Some commands (like the one above) only act as groupings for subcommands, like `from`, `to` or `doc`. Since they do not serve a
purpose on their own, they will render their help output automatically:

```
> doc
Usage:
  > doc

Subcommands:
  doc get - Fetches a document through the data service
  doc import - Import documents from a file through the data service
  doc insert - Insert a document through the data service
  doc remove - Removes a document through the data service
  doc replace - Replace a document through the data service
  doc upsert - Upsert (insert or override) a document through the data service

Flags:
  -h, --help - Display the help message for this command
```

=== The Prompt explained
Couchbase Shell uses a custom, two line prompt to show you exactly in what environment you are working in right now. Since you
can connect to different clusters, switch buckets etc. it is important to know what is currently "active", as the active buckets/scope/collection
will be what commands are run against. Here is a sample prompt that will greet you when starting the shell:

```
👤 Administrator 🏠 local in 🗄 travel-sample._default._default
>
```

It tells you that your user is `Administrator`, the current active cluster identifier is `local` and the active bucket is `travel-sample`.
The current scope and collection have not been set, so they assume the `_default` value.

If you have an active scope or collection set then the prompt will also update to reflect that:

```
👤 Administrator 🏠 local in 🗄 travel-sample.inventory.landmark
>
```

If we ran a `doc get` it would fetch the doc from travel-sample.inventory.landmark. In the second line, your actual user prompt starts.

=== Loading Data into the Shell

If you want to import data into Couchbase, or just load it into the shell for further processing, there are different commands available to help you.
Once the data is loaded into the shell it can be sent to one of the Couchbase save commands like `doc upsert` and `doc import`.
Depending on the structure of the data, and the command used, you may also need to tweak it a little bit so it can be properly stored.

==== Doc import

The `doc import` is the simplest way to import data through Couchbase Shell.
The command expects a path to file containing data, the data can be any of the formats supported by the `from` command.
In the following example we import a json file containing a single document.
We also specify the `--id-column` flag because `doc import` will try to use an `id` field for the document key by default.

```
> cat user.json
{
  "name": "Michael",
  "age": 32,
  "height": 180
}

> doc import user.json --id-column name
╭───┬───────────┬─────────┬────────┬──────────┬───────────╮
│ # │ processed │ success │ failed │ failures │  cluster  │
├───┼───────────┼─────────┼────────┼──────────┼───────────┤
│ 0 │         1 │       1 │      0 │          │ dev.local │
╰───┴───────────┴─────────┴────────┴──────────┴───────────╯
```

```
> doc import -h
Import documents from a file through the data service

Usage:
  > doc import {flags} <filename>

Flags:
  -h, --help - Display the help message for this command
  --id-column <String> - the name of the id column if used with an input stream
  --bucket <String> - the name of the bucket
  --expiry <Number> - the expiry for the documents in seconds, or absolute
  --scope <String> - the name of the scope
  --collection <String> - the name of the collection
  --clusters <String> - the clusters which should be contacted
  --batch-size <Number> - the maximum number of items to batch send at a time

Parameters:
  filename <string>: the path to the file containing data to import
```

==== Manual import

The `open` command will look at file endings and try to decode it automatically.
Imagine a file named `user.json` in your current directory.

```
> cat user.json
{
  "name": "Michael",
  "age": 32,
  "height": 180
}

> open user.json
╭────────┬─────────╮
│ name   │ Michael │
│ age    │ 32      │
│ height │ 180     │
╰────────┴─────────╯
```

As you can see, the `open` command automatically decoded the JSON document into the tabular format.
If the filename would only be `user`, the import would look like this instead:

```
> open user
{
  "name": "Michael",
  "age": 32,
  "height": 180
}
```

If you are dealing with data that cannot be decoded automatically, you can use the various `from` subcommands to help with decoding.
In our case we use `from json`:

```
> open user | from json
╭────────┬─────────╮
│ name   │ Michael │
│ age    │ 32      │
│ height │ 180     │
╰────────┴─────────╯
```

TIP: look at the many different import formats `from` supports, including csv, xml, yaml and even sqlite. With this simple tool
at hand you are able to load many different data formats quickly and import them into Couchbase!

We cannot use this format directly with commands like `doc upsert` as the command expects two "columns" in the data - id and content.
This means that we have to perform some translation from the above format to one that `doc upsert` understands.
To do this we `wrap` the entire document into a content column and then extract the id that we want to use:

```
> open user.json | wrap content | insert id $in.content.name
╭─────────┬──────────────────────╮
│         │ ╭────────┬─────────╮ │
│ content │ │ name   │ Michael │ │
│         │ │ age    │ 32      │ │
│         │ │ height │ 180     │ │
│         │ ╰────────┴─────────╯ │
│ id      │ Michael              │
╰─────────┴──────────────────────╯
```

There are many other approaches to achieving this same result.
With our data in the correct format we can then upsert:

```
> open user.json | wrap content | insert id $in.content.name | doc upsert
╭───┬───────────┬─────────┬────────┬──────────┬───────────╮
│ # │ processed │ success │ failed │ failures │  cluster  │
├───┼───────────┼─────────┼────────┼──────────┼───────────┤
│ 0 │         1 │       1 │      0 │          │ local     │
╰───┴───────────┴─────────┴────────┴──────────┴───────────╯
```

See the <<_importing_data,Importing data recipes>> for more information.

=== Exporting Data from the Shell

The export counterparts to `open` and `from`, are `save` and `to`.
You can use both commands to take tabular data from the shell and store it in files of the needed target format.

Like `open`, `save` will try to discern the format from the file ending.
The following example will load a JSON file and then save it as CSV:

```
> cat user.json
{
  "name": "Michael",
  "age": 32,
  "height": 180
}
```

```
> open user.json | save user.csv
```

```
> cat user.csv
name,age,height
Michael,32,180
```

This example is dealing with only one row for simplicity, but you can save as many rows as you need in one file.

As a motivating example, the following snippet runs a N1QL query and stores the result as a csv file:

```
> query "select airportname,city,country from `travel-sample` where type = 'airport' limit 10" | save output.csv
```

```
> cat output.csv
airportname,city,country
Calais Dunkerque,Calais,France
Peronne St Quentin,Peronne,France
Les Loges,Nangis,France
Couterne,Bagnole-de-l'orne,France
Bray,Albert,France
Le Touquet Paris Plage,Le Tourquet,France
Denain,Valenciennes,France
Glisy,Amiens,France
La Garenne,Agen,France
Cazaux,Cazaux,France
```

See the <<_exporting_data,Exporting data recipes>> for more information.
